# LLM Tokenization Strategies for Mathematical Equations

A systematic comparison of tokenization methods (character-level, BPE, and hierarchical) for training sequence models on synthetic mathematical equations. 
This project evaluates how tokenization choice impacts model performance across LSTM and Transformer architectures.

## ðŸ“‹ Overview

This project implements an end-to-end pipeline to:
1. **Generate** synthetic mathematical equations with controlled complexity
2. **Tokenize** equations using three different strategies
3. **Train** LSTM and Transformer models on tokenized data
4. **Evaluate** model performance across tokenization-architecture combinations
